{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e93446fa66ed5ab4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Reinforced Learning \n",
    "\n",
    "<p>Reinforced Learning is when a machine learns from experience and takes proper decisions in order to maximize its reward to get the best reward possible.<p>\n",
    "\n",
    "<p>Libraries used:\n",
    "\n",
    "import gymnasium as gym: This is the primary library used for reinforcement learning environments. It provides a variety of environments and tools to test and develop RL algorithms. Here, it's used to create and interact with the CartPole environment.\n",
    "\n",
    "<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b7639367e9ec0b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T09:21:00.115624Z",
     "start_time": "2024-08-22T09:20:59.908102Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7989282d038938",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialise the CartPole environment with human rendering\n",
    "env = gym.make('CartPole-v1', render_mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987d8a2ea2b43d25",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# returns an initial state\n",
    "(state, _) = env.reset()\n",
    "\n",
    "# State contains: cart position, cart velocity, pole angle, pole angular velocity\n",
    "\n",
    "# render the environment\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7900519abd22ba9f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test a single step in the environment by pushing the cart left\n",
    "env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fe6e22ce942eee",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "env.observation_space  # state space details\n",
    "env.observation_space.high  # max state values\n",
    "env.observation_space.low  # min state values\n",
    "env.action_space  # available actions\n",
    "env.spec  # environment specifications\n",
    "\n",
    "# check the environment's constraints\n",
    "env.spec.max_episode_steps\n",
    "\n",
    "# reward threshold per episode\n",
    "env.spec.reward_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f329561521ec9e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## State and Action spaces\n",
    "\n",
    "<p> Observation Space: What the state vector contains (e.g., cart position, velocity, pole angle).\n",
    "Action Space: The possible actions the agent can take (e.g., move left or right). <p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce449c543afdca44",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "episodeNumber = 1000\n",
    "timeSteps = 100 #steps per episode\n",
    "\n",
    "for episodeIndex in range(episodeNumber):\n",
    "    initial_state = env.reset()\n",
    "    print(episodeIndex) # track episode number\n",
    "    env.render()\n",
    "    appendedObservations = []\n",
    "    for timeIndex in range(timeSteps):\n",
    "        print(timeIndex)\n",
    "        random_action = env.action_space.sample()\n",
    "        observation, reward, terminated, truncated, info = env.step(random_action)\n",
    "        appendedObservations.append(observation)\n",
    "        if (terminated):\n",
    "            time.sleep(.5)\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dadbba57e8243c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<img src=\"https://images.ctfassets.net/xjan103pcp94/5h5ZwNAqLHAIRZ9jPGvRU1/6ceb65f718883cf2e7b8ca9dcd0a5fc4/Blog_-_intro_reinforcement_2.png\" width=\"50%\" />\n",
    "\n",
    "\n",
    "Flowchart of Simulation Loop:\n",
    "\n",
    "<p>\n",
    "[Start Simulation]\n",
    "  -> [For Each Episode]\n",
    "    -> [Reset Environment]\n",
    "    -> [For Each Time Step]\n",
    "      -> [Sample Random Action]\n",
    "      -> [Step Environment]\n",
    "      -> [Store Observation]\n",
    "      -> [Check Termination]\n",
    "    -> [Render]\n",
    "    -> [Pause if Terminated]\n",
    "  -> [Close Environment]\n",
    " <p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b282e99e4696a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<https://livebook.manning.com/concept/reinforcement-learning/cart-pole-environment>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
